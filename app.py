import streamlit as st
import os
import traceback
from pathlib import Path
from typing import List, Dict, Any, Optional

# --- Streamlit UI è¨­å®š (æœ€åˆã«å‘¼ã³å‡ºã™) ---
st.set_page_config(page_title="ğŸ“„ Chat with Document (Graph-RAG)", layout="wide")

from dotenv import load_dotenv

# LangChain / Google Gemini
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_experimental.graph_transformers import LLMGraphTransformer

# Neo4jGraph (langchain-neo4j ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰)
try:
    from langchain_neo4j import Neo4jGraph
except ImportError:
    st.error("langchain_neo4j ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install langchain-neo4j` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    Neo4jGraph = None

# PGVector
try:
    from langchain_community.vectorstores.pgvector import PGVector
except ImportError:
    try:
        from langchain_postgres import PGVector
    except ImportError:
        st.error("PGVector ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install langchain-community` ã¾ãŸã¯ `pip install langchain-postgres` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        PGVector = None

# Core LangChain components
# GraphDocument ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (æœ€æ–°ã®ãƒ‘ã‚¹ã‚’å„ªå…ˆã—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç”¨æ„)
try:
    from langchain_core.graph_document import GraphDocument
except ImportError:
    try:
        from langchain_community.graphs.graph_document import GraphDocument
    except ImportError:
        try:
            from langchain_experimental.graph_transformers.base import GraphDocument
        except ImportError:
            st.error("GraphDocument ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚langchain-coreã¾ãŸã¯langchain-communityã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            GraphDocument = None

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.documents import Document

# Neo4jå°‚ç”¨ã®QAãƒã‚§ãƒ¼ãƒ³ (æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å ´åˆ)
try:
    from langchain_neo4j import GraphCypherQAChain
except ImportError:
    try:
        from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain
    except ImportError:
        GraphCypherQAChain = None

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()

# Google Gemini
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# Neo4j
NEO4J_URI = os.getenv("NEO4J_URI")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PW = os.getenv("NEO4J_PW")

# PGVector
PG_CONN = os.getenv("PG_CONN")

st.title("ğŸ“„ Chat with Document (Graph-RAG & Gemini)")

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chain" not in st.session_state:
    st.session_state.chain = None
if "processed_file" not in st.session_state:
    st.session_state.processed_file = None

# --- Helper Functions ---

def format_graph_results(graph_data: List[Dict[str, Any]] | Any) -> str:
    """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not graph_data:
        return "ã‚°ãƒ©ãƒ•æƒ…å ±ã¯å–å¾—ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    if isinstance(graph_data, str):
        return graph_data
    
    if isinstance(graph_data, list):
        if not graph_data:
            return "æ•´å½¢å¯¾è±¡ã®ã‚°ãƒ©ãƒ•æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ (ç©ºãƒªã‚¹ãƒˆ)ã€‚"
        formatted_results = []
        for item in graph_data:
            if isinstance(item, dict):
                formatted_results.append(str(item))
            else:
                formatted_results.append(str(item))
        return "\n".join(formatted_results)
    
    return str(graph_data)

def format_document_results(docs: List[Document]) -> str:
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆã‚’æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if not docs:
        return "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯å–å¾—ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    formatted_docs = []
    for doc in docs:
        source_file = doc.metadata.get('source_file', 'N/A')
        chunk_id = doc.metadata.get('doc_id', 'N/A')
        content = doc.page_content
        
        formatted_doc = f"å‡ºå…¸ãƒ•ã‚¡ã‚¤ãƒ«: {source_file}, ãƒãƒ£ãƒ³ã‚¯ID: {chunk_id}\nå†…å®¹: {content}"
        formatted_docs.append(formatted_doc)
    
    return "\n---\n".join(formatted_docs)

def create_graph_retriever(graph_db, llm):
    """ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç”¨ã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆ"""
    if GraphCypherQAChain:
        try:
            # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®langchain-neo4jã®å ´åˆ
            qa_chain = GraphCypherQAChain.from_llm(
                llm=llm,
                graph=graph_db,
                verbose=True,
                return_intermediate_steps=False,
                allow_dangerous_requests=True  # å¿…è¦ã«å¿œã˜ã¦
            )
            
            def retriever_func(query):
                try:
                    result = qa_chain.invoke({"query": query})
                    if isinstance(result, dict):
                        return result.get("result", "")
                    return str(result)
                except Exception as e:
                    st.warning(f"ã‚°ãƒ©ãƒ•ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                    return ""
            
            return RunnableLambda(retriever_func)
        except Exception as e:
            st.warning(f"GraphCypherQAChainä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªCypherã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    def simple_graph_retriever(query):
        try:
            # :NEXTãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãŸã©ã‚‹ã‚¯ã‚¨ãƒªã®ä¾‹ã‚’è¿½åŠ 
            cypher_query = f"""
            MATCH (n)
            WHERE toLower(n.id) CONTAINS toLower($query) OR toLower(n.chunkId) CONTAINS toLower($query)
            // é–¢é€£ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’NEXTãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ãŸã©ã‚‹
            OPTIONAL MATCH (n)-[:NEXT*..2]->(related_chunk)
            RETURN n, related_chunk
            LIMIT 10
            """
            results = graph_db.query(cypher_query, params={"query": query})
            return str(results) if results else ""
        except Exception as e:
            st.warning(f"ã‚°ãƒ©ãƒ•ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return ""
    
    return RunnableLambda(simple_graph_retriever)

@st.cache_resource(show_spinner="ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­...")
def build_rag_chain(_raw_text, _file_name):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹"""
    try:
        # 1. Embeddings ã¨ LLM ã®åˆæœŸåŒ–
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001", 
            google_api_key=GOOGLE_API_KEY
        )
        
        llm_graph_extraction = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=GOOGLE_API_KEY,
            temperature=0
        )
        
        llm_answer_generation = ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-exp",
            google_api_key=GOOGLE_API_KEY,
            temperature=0.1
        )

        # 2. ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        documents = text_splitter.create_documents([_raw_text])
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        for i, doc in enumerate(documents):
            doc.metadata["doc_id"] = f"chunk_{_file_name}_{i}"
            doc.metadata["source_file"] = _file_name

        # 3. ã‚°ãƒ©ãƒ•æŠ½å‡ºã¨Neo4jã¸ã®ãƒ­ãƒ¼ãƒ‰
        graph_retriever_runnable = RunnableLambda(lambda x: "")
        
        if Neo4jGraph and documents:
            transformer = LLMGraphTransformer(llm=llm_graph_extraction)
            try:
                # ã‚°ãƒ©ãƒ•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¤‰æ›
                graph_documents = transformer.convert_to_graph_documents(documents)
                
                # Neo4jã«æ¥ç¶š
                graph_db = Neo4jGraph(
                    url=NEO4J_URI, 
                    username=NEO4J_USER, 
                    password=NEO4J_PW,
                    refresh_schema=False  # ã‚¹ã‚­ãƒ¼ãƒã®è‡ªå‹•æ›´æ–°ã‚’ç„¡åŠ¹åŒ–
                )
                
                # ã‚°ãƒ©ãƒ•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
                graph_db.add_graph_documents(
                    graph_documents,
                    baseEntityLabel=True,
                    include_source=True
                )

                # ãƒãƒ£ãƒ³ã‚¯ãƒãƒ¼ãƒ‰ã®æƒ…å ±ã‚’æ›´æ–°
                st.sidebar.info("ãƒãƒ£ãƒ³ã‚¯ãƒãƒ¼ãƒ‰æƒ…å ±ã‚’æ›´æ–°ä¸­...")
                for i, doc in enumerate(documents):
                    source_id = doc.page_content
                    chunk_id = doc.metadata["doc_id"]
                    chunk_name = f"Chunk {i}: {source_id[:30]}..."
                    graph_db.query(
                        """
                        MATCH (d:Document {id: $source_id})
                        SET d:Chunk, d.chunkId = $chunk_id, d.text = $text, d.name = $name
                        REMOVE d.id
                        """,
                        params={
                            "source_id": source_id,
                            "chunk_id": chunk_id,
                            "text": source_id,
                            "name": chunk_name
                        }
                    )

                # ãƒãƒ£ãƒ³ã‚¯é–“ã® :NEXT ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
                st.sidebar.info("ãƒãƒ£ãƒ³ã‚¯é–“ã®é–¢é€£ã‚’ä½œæˆä¸­...")
                if len(documents) > 1:
                    for i in range(len(documents) - 1):
                        graph_db.query(
                            """
                            MATCH (c1:Chunk {chunkId: $chunk_id_1})
                            MATCH (c2:Chunk {chunkId: $chunk_id_2})
                            MERGE (c1)-[:NEXT]->(c2)
                            """,
                            params={
                                "chunk_id_1": documents[i].metadata["doc_id"],
                                "chunk_id_2": documents[i+1].metadata["doc_id"]
                            }
                        )
                
                # ã‚¹ã‚­ãƒ¼ãƒã‚’æ‰‹å‹•ã§æ›´æ–°
                graph_db.refresh_schema()
                
                # ã‚°ãƒ©ãƒ•ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä½œæˆ
                graph_retriever_runnable = create_graph_retriever(graph_db, llm_answer_generation)
                
                st.sidebar.success("Neo4jã«ã‚°ãƒ©ãƒ•ã¨ãƒãƒ£ãƒ³ã‚¯é–¢é€£æ€§ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                st.sidebar.error(f"Neo4jå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                st.sidebar.error(traceback.format_exc())

        # 4. PGVectorã¸ã®ä¿å­˜ã¨Retrieveræ§‹ç¯‰
        vector_retriever_runnable = RunnableLambda(lambda x: [])
        
        if PGVector and documents:
            try:
                # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                collection_name = f"graph_rag_{_file_name.replace('.', '_').replace(' ', '_').lower()}"
                
                vector_store = PGVector.from_documents(
                    documents=documents,
                    embedding=embeddings,
                    connection_string=PG_CONN,
                    collection_name=collection_name,
                    pre_delete_collection=True,
                )
                
                vector_retriever_runnable = vector_store.as_retriever(
                    search_kwargs={"k": 5}
                )
                
                st.sidebar.success("PGVectorã«ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
            except Exception as e:
                st.sidebar.error(f"PGVectorå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                st.sidebar.error(traceback.format_exc())

        # 5. LCEL ãƒã‚§ãƒ¼ãƒ³å®šç¾©
        retrieve_context = RunnableParallel({
            "graph_context": graph_retriever_runnable | RunnableLambda(format_graph_results),
            "document_context": vector_retriever_runnable | RunnableLambda(format_document_results),
            "question": RunnablePassthrough()
        })

        prompt_template_str = """ã‚ãªãŸã¯æä¾›ã•ã‚ŒãŸæƒ…å ±ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹å°‚é–€å®¶AIã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’åˆ©ç”¨ã—ã¦ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

## ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{graph_context}

## ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{document_context}

---
ä¸Šè¨˜ã®æƒ…å ±ã®ã¿ã‚’æ ¹æ‹ ã¨ã—ã¦ã€ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€æ—¥æœ¬èªã§ç¶²ç¾…çš„ã‹ã¤æ­£ç¢ºã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {question}
å›ç­”:"""
        
        prompt = PromptTemplate.from_template(prompt_template_str)

        chain = (
            retrieve_context 
            | prompt 
            | llm_answer_generation 
            | StrOutputParser()
        )
        
        return chain
        
    except Exception as e:
        st.error(f"RAGãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.error(traceback.format_exc())
        return None

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header("è¨­å®š")
    
    # ç’°å¢ƒå¤‰æ•°ã®ãƒã‚§ãƒƒã‚¯
    st.subheader("ç’°å¢ƒå¤‰æ•°ã®çŠ¶æ…‹")
    env_vars = {
        "GOOGLE_API_KEY": bool(GOOGLE_API_KEY),
        "NEO4J_URI": bool(NEO4J_URI),
        "NEO4J_USER": bool(NEO4J_USER),
        "NEO4J_PW": bool(NEO4J_PW),
        "PG_CONN": bool(PG_CONN)
    }
    
    for var, is_set in env_vars.items():
        if is_set:
            st.success(f"âœ… {var}")
        else:
            st.error(f"âŒ {var}")
    
    # ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒã‚§ãƒƒã‚¯
    st.subheader("å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸")
    packages = {
        "langchain-neo4j": Neo4jGraph is not None,
        "PGVector": PGVector is not None,
        "GraphDocument": GraphDocument is not None
    }
    
    for package, is_available in packages.items():
        if is_available:
            st.success(f"âœ… {package}")
        else:
            st.error(f"âŒ {package}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ« (.txt) ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", 
        type=["txt"]
    )

    if uploaded_file is not None:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰å‡¦ç†ã‚’é–‹å§‹
        if st.session_state.processed_file != uploaded_file.name:
            st.session_state.processed_file = uploaded_file.name
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
            try:
                raw_text = uploaded_file.getvalue().decode("utf-8")
            except UnicodeDecodeError:
                raw_text = uploaded_file.getvalue().decode("utf-8", errors="ignore")
                st.warning("ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
            # ç’°å¢ƒå¤‰æ•°ã®ãƒã‚§ãƒƒã‚¯
            if not all([GOOGLE_API_KEY, NEO4J_URI, NEO4J_PW, PG_CONN]):
                st.error(".envãƒ•ã‚¡ã‚¤ãƒ«ã«å¿…è¦ãªè¨­å®šãŒã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            else:
                # RAGãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰
                with st.spinner("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­..."):
                    chain = build_rag_chain(raw_text, uploaded_file.name)
                    if chain:
                        st.session_state.chain = chain
                        st.success(f"`{uploaded_file.name}` ã®æº–å‚™ãŒã§ãã¾ã—ãŸï¼")
                        # æº–å‚™ãŒã§ããŸã‚‰ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¯ãƒªã‚¢
                        st.session_state.messages = []
                    else:
                        st.error("RAGãƒã‚§ãƒ¼ãƒ³ã®æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

# --- ãƒ¡ã‚¤ãƒ³ãƒãƒ£ãƒƒãƒˆç”»é¢ ---

# éå»ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æ–°ã—ã„å…¥åŠ›ã‚’å¾…ã¤
if prompt := st.chat_input("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„..."):
    if st.session_state.chain is None:
        st.warning("ã¾ãšã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†ã‚’å®Œäº†ã•ã›ã¦ãã ã•ã„ã€‚")
    else:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å±¥æ­´ã«è¿½åŠ ã—ã¦è¡¨ç¤º
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”ã‚’ç”Ÿæˆ
        with st.chat_message("assistant"):
            with st.spinner("å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                try:
                    response = st.session_state.chain.invoke(prompt)
                    st.markdown(response)
                    # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å›ç­”ã‚’å±¥æ­´ã«è¿½åŠ 
                    st.session_state.messages.append({"role": "assistant", "content": response})
                except Exception as e:
                    error_message = f"å›ç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\n{traceback.format_exc()}"
                    st.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
